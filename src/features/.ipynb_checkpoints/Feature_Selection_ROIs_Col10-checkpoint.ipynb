{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bd4272",
   "metadata": {
    "id": "41bd4272"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvSYfDDhGr_G",
   "metadata": {
    "id": "fvSYfDDhGr_G"
   },
   "source": [
    "### imports packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bd0253",
   "metadata": {
    "id": "e1bd0253"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from numpy import set_printoptions\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import manifold, datasets\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d435efa",
   "metadata": {
    "id": "2d435efa"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SAMR3bGOjdCj",
   "metadata": {
    "id": "SAMR3bGOjdCj"
   },
   "source": [
    "### loading Data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uHoK4uvAjwHS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHoK4uvAjwHS",
    "outputId": "250437e1-6ffd-42b1-e4a8-0d2e3edda425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7619.csv\n",
      "#3 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7712.csv\n",
      "#4 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/765.csv\n",
      "#6 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7746.csv\n",
      "#10 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7615.csv\n",
      "#11 >> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/773.csv\n"
     ]
    }
   ],
   "source": [
    "path_base = '/home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2'\n",
    "pathFeaturesBase = '/home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect'\n",
    "lstfiles = glob.glob(path_base + '/*')\n",
    "# print(lstfiles)\n",
    "lstpathfiles = []\n",
    "for cc, npath in enumerate(lstfiles):\n",
    "    if 'rois_grade' not in npath:\n",
    "        print(f\"#{cc} >> {npath}\")\n",
    "        lstpathfiles.append(npath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyvBo8CuHAh8",
   "metadata": {
    "id": "wyvBo8CuHAh8"
   },
   "source": [
    "### Load Tables and paramenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "v8Pq2ueXr4tz",
   "metadata": {
    "id": "v8Pq2ueXr4tz"
   },
   "outputs": [],
   "source": [
    "# read many files CSVs\n",
    "manyfile = False\n",
    "filterYear = True\n",
    "nyear = 2023\n",
    "# lstpathfiles = glob.glob(os.path.join(path_base, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diyoNFd9fiNP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diyoNFd9fiNP",
    "outputId": "21c7d78e-0a45-44ba-8565-71b021c053b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "dftmp = pd.read_csv(lstpathfiles[4])\n",
    "dftmp = dftmp.drop(['GRID_ID','system:index','.geo'], axis=1)\n",
    "lstCol = list(dftmp.columns)\n",
    "print(len(lstCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Vs6UUNurhu7M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vs6UUNurhu7M",
    "outputId": "0826fd5b-1976-4ef0-a3a2-f4da035eaa1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
      "\"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
      "\"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
      "\"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
      "\"bsi_median_2\",\"class\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
      "\"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
      "\"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
      "\"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
      "\"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
      "\"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
      "\"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
      "\"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
      "\"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
      "\"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
      "\"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
      "\"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
      "\"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
      "\"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
      "\"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
      "\"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
      "\"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
      "\"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\"year\",\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for ii in range(0, len(lstCol)):\n",
    "    text = text + f'\"{lstCol[ii]}\",'\n",
    "    if ii % 5 == 0 and ii > 0:\n",
    "        print(text)\n",
    "        text = ''\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1HeNYU0t36Mi",
   "metadata": {
    "id": "1HeNYU0t36Mi"
   },
   "source": [
    "### Analises de Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eGLUTvyx1ooX",
   "metadata": {
    "id": "eGLUTvyx1ooX"
   },
   "outputs": [],
   "source": [
    "class processin_features_byYears(object):\n",
    "    columns_features = [\n",
    "        \"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
    "        \"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
    "        \"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
    "        \"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
    "        \"bsi_median_2\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
    "        \"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
    "        \"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
    "        \"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
    "        \"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
    "        \"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
    "        \"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
    "        \"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
    "        \"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
    "        \"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
    "        \"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
    "        \"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
    "        \"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
    "        \"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
    "        \"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
    "        \"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
    "        \"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
    "        \"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\n",
    "    ]\n",
    "    classe = \"class\"\n",
    "\n",
    "    def __init__(self, Ns_estimators, learning_rates, path_features):\n",
    "        self.dfROIs = None\n",
    "        self.dfCC = None\n",
    "        self.yearAct = None\n",
    "        self.lstClass = None\n",
    "        self.lst_N_estimators = Ns_estimators\n",
    "        self.lst_learning_rate = learning_rates\n",
    "        self.path_features = path_features\n",
    "        self.betterPmtrosSet = 0\n",
    "        self.dictpmtGTB = {}\n",
    "        count = 0\n",
    "        for ne in self.lst_N_estimators:\n",
    "            for lr in self.lst_learning_rate:\n",
    "                self.dictpmtGTB[str(count)] = [ne, lr]\n",
    "                print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
    "                count += 1\n",
    "\n",
    "    def get_data(self, myDF, nYear):\n",
    "        self.dfROIs = myDF\n",
    "        self.yearAct = nYear\n",
    "        self.lstClass = self.dfROIs[self.classe].unique().tolist()\n",
    "        self.buildingPercentsofClass()\n",
    "\n",
    "    def get_class_withSmallsize(self, dFrames, lstSearch):\n",
    "        classeMin = []\n",
    "        for cclass in lstSearch:\n",
    "            nsize = dFrames[dFrames[self.classe] == cclass].shape[0]\n",
    "            print(f\" classe {cclass} == > size = {nsize}\")\n",
    "            if nsize < 4:\n",
    "                classeMin.append(cclass)\n",
    "        return classeMin\n",
    "\n",
    "    def split_dataFrame(self, dFrame):\n",
    "        # split data into inputs (X) and outputs (y)\n",
    "        dFrame4 = dFrame[dFrame[self.classe] == 4]\n",
    "        dFrameO = dFrame[dFrame[self.classe] != 4]\n",
    "        # lstClasses  = [kk for kk in self.lstClass if kk != 4]\n",
    "        addFeatext = False\n",
    "\n",
    "        maximoROIs = self.dfCC[self.dfCC['class'] != 4]['count'].max()\n",
    "        maximoROIs += 150\n",
    "        newlstDF = []\n",
    "        print(\"size dFrame4 \", dFrame4.shape, \" and the next class maximum is \", maximoROIs)\n",
    "        # sampled the N samples fro dataframe stratified\n",
    "        tmpDF = dFrame4.sample(n= int(maximoROIs), random_state= np.random.seed(int(maximoROIs/ 2)), replace= True)  #\n",
    "        concDF  = pd.concat([tmpDF, dFrameO], ignore_index=True) #\n",
    "        print(\"temos {} filas \".format(concDF.shape))\n",
    "        # concDF.head()\n",
    "        lstCCg1 = [3,4,15,18]\n",
    "        lstCCg2 = [12,21,22,33]\n",
    "        print(\" ====> analisando size of class smaller \")\n",
    "        lstclassMinM = self.get_class_withSmallsize(dFrame, lstCCg2)\n",
    "\n",
    "        if len(lstclassMinM)  > 0:\n",
    "            for ccm in lstclassMinM:\n",
    "                print(\" --- will be remove class ---\", ccm)\n",
    "                lstCCg2.remove(ccm)\n",
    "            addFeatext = True\n",
    "\n",
    "\n",
    "        dFrameg1 = concDF[concDF[self.classe].isin(lstCCg1)]\n",
    "        dFrameg2 = concDF[concDF[self.classe].isin(lstCCg2)]\n",
    "\n",
    "        # print(f\" adding {int(propCC * maximoROIs)} samples from class [{cclass}]\")\n",
    "        # X = dataFrame[self.columns_features[:]]\n",
    "        # y = dataFrame[self.classe]\n",
    "        X_traing1, X_testg1, y_traing1, y_testg1 = train_test_split(\n",
    "                            dFrameg1[self.columns_features[:]], dFrameg1[self.classe],\n",
    "                            train_size=0.05,\n",
    "                            random_state=1,\n",
    "                            shuffle=True,\n",
    "                            stratify = dFrameg1[self.classe]\n",
    "                        )\n",
    "        print(f\"colected Xtrain {X_traing1.shape[0]} | Xtest {X_testg1.shape[0]} | \" +\n",
    "                                f\"ytrain {y_traing1.shape[0]} | ytest {y_testg1.shape[0]}\")\n",
    "        X_traing2, X_testg2, y_traing2, y_testg2 = train_test_split(\n",
    "                            dFrameg2[self.columns_features[:]], dFrameg2[self.classe],\n",
    "                            train_size=0.9,\n",
    "                            random_state=1,\n",
    "                            shuffle=True,\n",
    "                            stratify = dFrameg2[self.classe]\n",
    "                        )\n",
    "        print(f\"colected Xtrain {X_traing2.shape[0]} | Xtest {X_testg2.shape[0]} | \" +\n",
    "                                f\"ytrain {y_traing2.shape[0]} | ytest {y_testg2.shape[0]}\")\n",
    "\n",
    "\n",
    "        self.X_train = pd.concat([X_traing1, X_traing2], ignore_index=True)\n",
    "        self.X_test = pd.concat([X_testg1, X_testg2], ignore_index=True)\n",
    "        self.y_train = pd.concat([y_traing1, y_traing2], ignore_index=True)\n",
    "        self.y_test = pd.concat([y_testg1, y_testg2], ignore_index=True)\n",
    "\n",
    "        print(f\" ==== know we have {self.X_train.shape} to train ==== \")\n",
    "        print(self.y_train.value_counts(normalize= True), self.y_train.value_counts())\n",
    "\n",
    "\n",
    "    def buildingPercentsofClass(self):\n",
    "        self.dfCC = self.dfROIs['class'].value_counts()\n",
    "        self.dfCC = self.dfCC.reset_index()\n",
    "        # get total number of classes\n",
    "        total = np.sum(self.dfCC['count'].tolist())\n",
    "        print(f\"total = {total}\")\n",
    "        self.dfCC['percent'] = round((self.dfCC['count'] * 100)/ total, 2)\n",
    "        self.dfCC['Years'] = np.ones(self.dfCC.shape[0]).astype(int) * self.yearAct\n",
    "        # print(dfCC)\n",
    "        print(f\" == the CLASS of rois distribuided in {self.yearAct} are == \\n \", self.dfCC)\n",
    "\n",
    "    def processingMultiplesModels(self):\n",
    "        # split the dataframe in stratify samples by class and balance class 4\n",
    "        self.split_dataFrame(self.dfROIs)\n",
    "        maximAcc = 0.0\n",
    "        # get the models to evaluate\n",
    "        models = self.get_models()\n",
    "        # evaluate the models and store results\n",
    "        results, names = list(), list()\n",
    "        start = time.time()\n",
    "        count = 1\n",
    "        for name, model in models.items():\n",
    "            print(f\"#{count}/{len(models.items())} processing model {name}\")\n",
    "            scores = self.evaluate_model(model, self.X_train[self.columns_features[:]], self.y_train)\n",
    "            results.append(scores)\n",
    "            names.append(name)\n",
    "            print('  >%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "            if maximAcc < np.mean(scores):\n",
    "                self.betterPmtrosSet = count - 1\n",
    "                maximAcc = np.mean(scores)\n",
    "            count += 1\n",
    "        # plot model performance for comparison\n",
    "        # plt.boxplot(results, labels=names, showmeans=True)\n",
    "        # plt.show()\n",
    "        end = time.time()\n",
    "        tiempo = end - start\n",
    "        if tiempo < 60:\n",
    "            print(f\"model trained in {tiempo} seconds\")\n",
    "        else:\n",
    "            print(f\"model trained in {tiempo/60} minutos\")\n",
    "\n",
    "\n",
    "    # get a list of models to evaluate\n",
    "    def get_models(self):\n",
    "        min_features_to_select = 7\n",
    "        models = dict()\n",
    "\n",
    "        # criando pipeline do modelos gradiente Boosting com varios paramentros\n",
    "        cv = StratifiedKFold(3)\n",
    "        for cc in range(len(self.dictpmtGTB.keys())):\n",
    "            GTBmodel = GradientBoostingClassifier(\n",
    "                            n_estimators= self.dictpmtGTB[str(cc)][0],\n",
    "                            learning_rate= self.dictpmtGTB[str(cc)][1],\n",
    "                            max_features= 7\n",
    "                        )\n",
    "            rfe = RFECV(\n",
    "                    estimator=GTBmodel,\n",
    "                    step=1,\n",
    "                    cv=cv,\n",
    "                    scoring=\"accuracy\",\n",
    "                    min_features_to_select=min_features_to_select,\n",
    "                    n_jobs= -1,\n",
    "                )\n",
    "\n",
    "            models[str(cc)] = Pipeline(steps=[('s', rfe),('m', GTBmodel)])\n",
    "        return models\n",
    "\n",
    "    # evaluate a give model using cross-validation\n",
    "    def evaluate_model(self, model, X, y):\n",
    "        cv = StratifiedKFold(3)\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv,  error_score='raise') # n_jobs=-1,\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def get_better_featuresSet(self, fixarNumbFeat, numbMin, regg, yyear):\n",
    "\n",
    "        # Create a Path object\n",
    "        path_name_file = self.path_features + f'/featuresSelectS2_{regg}_{yyear}.csv'\n",
    "        file_path = Path(path_name_file)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if file_path.exists():\n",
    "            print(\" ******* list of features selected was saved ********\")\n",
    "\n",
    "        else:\n",
    "            self.split_dataFrame(self.dfROIs)\n",
    "\n",
    "            GTBmodel = GradientBoostingClassifier(\n",
    "                            n_estimators= self.dictpmtGTB[str(self.betterPmtrosSet)][0],\n",
    "                            learning_rate= self.dictpmtGTB[str(self.betterPmtrosSet)][1],\n",
    "                            max_features= 7,\n",
    "                            random_state=42\n",
    "                        )\n",
    "\n",
    "            start = time.time()\n",
    "            # Minimum number of features to consider\n",
    "            min_features_to_select =  7\n",
    "            cv = StratifiedKFold(3)\n",
    "            rfecv = RFECV(\n",
    "                estimator=GTBmodel,\n",
    "                step=1,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\",\n",
    "                min_features_to_select=min_features_to_select,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            rfecv.fit(self.X_train[self.columns_features], self.y_train)\n",
    "            print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "\n",
    "            end = time.time()\n",
    "            tiempo = end - start\n",
    "            if tiempo < 60:\n",
    "                print(f\"model trained in {tiempo} seconds\")\n",
    "            else:\n",
    "                print(f\"model trained in {tiempo/60} minutos\")\n",
    "\n",
    "            ## valores ótimos aparecem com valor 1 no ranking\n",
    "            lst_ranking = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < 2]\n",
    "            print(\"quantos features otimos \", len(lst_ranking))\n",
    "            numbNotOti = numbMin\n",
    "            if fixarNumbFeat and numbMin > len(lst_ranking):\n",
    "                numbNotOti = numbMin - len(lst_ranking)\n",
    "                print(f\"Addicionando << {numbNotOti} >> features a mais não ótimas \")\n",
    "                lst_ranking_tmp = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < numbNotOti]\n",
    "                lst_ranking += lst_ranking_tmp\n",
    "            print(\"quantos features otimos \", len(lst_ranking_tmp))\n",
    "                \n",
    "\n",
    "            lstFeatSelect = []\n",
    "            ccount = 1\n",
    "            for kk, cc in lst_ranking:\n",
    "                print(f\"# {ccount} ranking {kk} | pos {cc} >> feature >> {self.columns_features[cc]}\")\n",
    "                lstFeatSelect.append(self.columns_features[cc])\n",
    "                ccount += 1\n",
    "\n",
    "            dict_result= {\n",
    "                'ranking': lst_ranking,\n",
    "                'features': lstFeatSelect\n",
    "            }\n",
    "            dfresult = pd.DataFrame.from_dict(dict_result)\n",
    "            os.system(f\"mkdir {path_name_file}\") \n",
    "            os.chmod(path_name_file, 777)                   \n",
    "            dfresult.to_csv(path_name_file, index= False)\n",
    "\n",
    "            print(f\"tabela {path_name_file.split('/')[-1]} salva na pasta de output folder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "yW0vlol_H1VQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yW0vlol_H1VQ",
    "outputId": "890db644-5879-41a2-d0cf-7b843f85ffc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " year  [2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998, 1997, 1996, 1995, 1994, 1993, 1992, 1991, 1990, 1989, 1988, 1987, 1986]\n"
     ]
    }
   ],
   "source": [
    "lstYear = list(range(2024, 1985, -1))\n",
    "print(\" year \", lstYear);\n",
    "# sys.exit()\n",
    "# fixar o número de variaveis\n",
    "fixarNFeat = True\n",
    "# número máximo de variaveis para o modelo das 144\n",
    "numMin = 70\n",
    "yyear = 2023\n",
    "lstEstimadors = [15, 20, 30, 40, 50, 60]\n",
    "lstLearnRate = [0.001, 0.005, 0.01, 0.1]\n",
    "melhorModelo = 0\n",
    "dictModel = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0AXgPLMGfarP",
   "metadata": {
    "id": "0AXgPLMGfarP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0UGNM1nFhwuT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UGNM1nFhwuT",
    "outputId": "b522391d-fea7-4a16-a161-eb4913da41ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
      "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
      "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
      "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
      "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
      "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
      "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
      "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
      "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
      "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
      "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
      "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
      "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
      "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
      "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
      "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
      "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
      "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
      "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
      "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
      "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
      "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
      "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
      "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n",
      " os arquivos gerados vão se salvos aqui \n",
      " >>  /home/superuser/Dados/mapbiomas/dadosCol10/fileFeatSelect\n"
     ]
    }
   ],
   "source": [
    "# instanciar classe de processamento dos Features\n",
    "procFeatures_byYears = processin_features_byYears(lstEstimadors, lstLearnRate, pathFeaturesBase)\n",
    "print(\" os arquivos gerados vão se salvos aqui \\n >> \", procFeatures_byYears.path_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "i5tqkc1mh8mn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5tqkc1mh8mn",
    "outputId": "6edca6a6-d398-4d1b-87e0-2e7dd4a572b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate dictModel => {}\n",
      "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
      "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
      "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
      "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
      "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
      "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
      "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
      "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
      "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
      "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
      "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
      "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
      "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
      "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
      "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
      "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
      "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
      "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
      "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
      "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
      "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
      "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
      "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
      "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n"
     ]
    }
   ],
   "source": [
    "dictModelsS = {}\n",
    "pathModelJson = '/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1.json'\n",
    "try:\n",
    "    with open(pathModelJson, 'r') as fh:\n",
    "        dictModelsS = json.load(fh)\n",
    "    print(\"loaded dictModel => \", dictModelsS)\n",
    "    print(f\"with {len(dictModelsS.keys())} register\")\n",
    "except:\n",
    "    print(\"generate dictModel => {}\")\n",
    "\n",
    "count = 0\n",
    "for ne in lstEstimadors:\n",
    "    for lr in lstLearnRate:\n",
    "        dictModelsS[str(count)] = [ne, lr]\n",
    "        print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gjCdWlyRoP3K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjCdWlyRoP3K",
    "outputId": "98590d54-7809-4df0-a8ac-ef8812e2a39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [15, 0.001],\n",
       " '1': [15, 0.005],\n",
       " '2': [15, 0.01],\n",
       " '3': [15, 0.1],\n",
       " '4': [20, 0.001],\n",
       " '5': [20, 0.005],\n",
       " '6': [20, 0.01],\n",
       " '7': [20, 0.1],\n",
       " '8': [30, 0.001],\n",
       " '9': [30, 0.005],\n",
       " '10': [30, 0.01],\n",
       " '11': [30, 0.1],\n",
       " '12': [40, 0.001],\n",
       " '13': [40, 0.005],\n",
       " '14': [40, 0.01],\n",
       " '15': [40, 0.1],\n",
       " '16': [50, 0.001],\n",
       " '17': [50, 0.005],\n",
       " '18': [50, 0.01],\n",
       " '19': [50, 0.1],\n",
       " '20': [60, 0.001],\n",
       " '21': [60, 0.005],\n",
       " '22': [60, 0.01],\n",
       " '23': [60, 0.1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictModelsS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "KwKMr4zFH0dM",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwKMr4zFH0dM",
    "outputId": "3837b2e1-32af-4292-eac6-af3d394b2352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ==> /home/superuser/Dados/mapbiomas/dadosCol10/ROIsv2/ROIsv2/7746.csv\n",
      "  columns =  Index(['GRID_ID', 'afvi_median', 'afvi_median_dry', 'afvi_median_wet',\n",
      "       'avi_median', 'avi_median_dry', 'avi_median_wet', 'awei_median',\n",
      "       'awei_median_dry', 'awei_median_wet',\n",
      "       ...\n",
      "       'swir2_median_dry', 'swir2_median_wet', 'swir2_stdDev', 'ui_median',\n",
      "       'ui_median_dry', 'ui_median_wet', 'wetness_median',\n",
      "       'wetness_median_dry', 'wetness_median_wet', 'year'],\n",
      "      dtype='object', length=111)\n",
      " == Know how many rois have in the table == \n",
      "  year\n",
      "1999    2170\n",
      "1994    2135\n",
      "2017    2135\n",
      "2019    2100\n",
      "2018    2100\n",
      "1998    2100\n",
      "2020    2065\n",
      "1996    2065\n",
      "2010    2065\n",
      "2013    2065\n",
      "2009    2065\n",
      "2023    2030\n",
      "2012    2030\n",
      "1997    2030\n",
      "2000    2030\n",
      "2024    2030\n",
      "2008    2030\n",
      "1992    2030\n",
      "2021    2030\n",
      "2022    2030\n",
      "1995    1995\n",
      "1993    1995\n",
      "1986    1960\n",
      "1985    1960\n",
      "2007    1925\n",
      "2006    1925\n",
      "2011    1925\n",
      "2014    1925\n",
      "2004    1925\n",
      "2005    1925\n",
      "2003    1925\n",
      "2016    1890\n",
      "2015    1855\n",
      "2001    1820\n",
      "1990    1820\n",
      "1991    1820\n",
      "2002    1785\n",
      "1987    1785\n",
      "1989    1715\n",
      "1988    1575\n",
      "Name: count, dtype: int64\n",
      "total = 2030\n",
      " == the CLASS of rois distribuided in 2023 are == \n",
      "     class  count  percent  Years\n",
      "0      4   1015    50.00   2023\n",
      "1     15    420    20.69   2023\n",
      "2      3    315    15.52   2023\n",
      "3     12    140     6.90   2023\n",
      "4     21    105     5.17   2023\n",
      "5     22     35     1.72   2023\n",
      " basin activate 7746 and the other Regions (2030, 111) processadas \n",
      " ==>  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
      "size dFrame4  (1015, 111)  and the next class maximum is  570\n",
      "temos (1585, 111) filas \n",
      " ====> analisando size of class smaller \n",
      " classe 12 == > size = 140\n",
      " classe 21 == > size = 105\n",
      " classe 22 == > size = 35\n",
      " classe 33 == > size = 0\n",
      " --- will be remove class --- 33\n",
      "colected Xtrain 65 | Xtest 1240 | ytrain 65 | ytest 1240\n",
      "colected Xtrain 252 | Xtest 28 | ytrain 252 | ytest 28\n",
      " ==== know we have (317, 108) to train ==== \n",
      "class\n",
      "12    0.397476\n",
      "21    0.299685\n",
      "22    0.097792\n",
      "4     0.088328\n",
      "15    0.066246\n",
      "3     0.050473\n",
      "Name: proportion, dtype: float64 class\n",
      "12    126\n",
      "21     95\n",
      "22     31\n",
      "4      28\n",
      "15     21\n",
      "3      16\n",
      "Name: count, dtype: int64\n",
      "#1/24 processing model 0\n",
      "  >0 0.397 (0.002)\n",
      "#2/24 processing model 1\n",
      "  >1 0.697 (0.007)\n",
      "#3/24 processing model 2\n",
      "  >2 0.697 (0.007)\n",
      "#4/24 processing model 3\n",
      "  >3 0.972 (0.015)\n",
      "#5/24 processing model 4\n",
      "  >4 0.397 (0.002)\n",
      "#6/24 processing model 5\n",
      "  >5 0.697 (0.007)\n",
      "#7/24 processing model 6\n",
      "  >6 0.697 (0.007)\n",
      "#8/24 processing model 7\n",
      "  >7 0.972 (0.008)\n",
      "#9/24 processing model 8\n",
      "  >8 0.397 (0.002)\n",
      "#10/24 processing model 9\n",
      "  >9 0.697 (0.007)\n",
      "#11/24 processing model 10\n",
      "  >10 0.927 (0.005)\n",
      "#12/24 processing model 11\n",
      "  >11 0.984 (0.009)\n",
      "#13/24 processing model 12\n",
      "  >12 0.397 (0.002)\n",
      "#14/24 processing model 13\n",
      "  >13 0.697 (0.007)\n",
      "#15/24 processing model 14\n",
      "  >14 0.937 (0.012)\n",
      "#16/24 processing model 15\n",
      "  >15 0.968 (0.004)\n",
      "#17/24 processing model 16\n",
      "  >16 0.397 (0.002)\n",
      "#18/24 processing model 17\n",
      "  >17 0.845 (0.016)\n",
      "#19/24 processing model 18\n",
      "  >18 0.949 (0.022)\n",
      "#20/24 processing model 19\n",
      "  >19 0.975 (0.009)\n",
      "#21/24 processing model 20\n",
      "  >20 0.397 (0.002)\n",
      "#22/24 processing model 21\n",
      "  >21 0.921 (0.012)\n",
      "#23/24 processing model 22\n",
      "  >22 0.953 (0.008)\n",
      "#24/24 processing model 23\n",
      "  >23 0.972 (0.021)\n",
      "model trained in 32.70019026597341 minutos\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1_7746.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m dictModelsS[nbacia] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetter_pmtSet\u001b[39m\u001b[38;5;124m'\u001b[39m: melhorModelo,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: dictModelsS[\u001b[38;5;28mstr\u001b[39m(melhorModelo)][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: dictModelsS[\u001b[38;5;28mstr\u001b[39m(melhorModelo)][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Convert and write JSON object to file\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpathModelJson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m     27\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkdir \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathModelJson\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# os.system(f'chmod 777 {pathModelJson}')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1_7746.json'"
     ]
    }
   ],
   "source": [
    "for cc, npath in enumerate(lstpathfiles[3:]):\n",
    "    df_bacia = {}    \n",
    "    print(\"Loading ==> \" + npath)\n",
    "    nbacia = npath.split(\"/\")[-1].replace('.csv', '')    \n",
    "    pathModelJson = f'/home/superuser/Dados/mapbiomas/dadosCol10/dictBetterModelpmtCol10v1_{nbacia}.json'\n",
    "    os.system(f\"mkdir {pathModelJson}\") \n",
    "    # os.system(f'chmod 777 {pathModelJson}')\n",
    "    os.chmod(pathModelJson, 777)\n",
    "    dftable = pd.read_csv(npath)\n",
    "    dftable = dftable.drop(['system:index','.geo'], axis=1)\n",
    "    print(\"  columns = \", dftable.columns)\n",
    "    print(f\" == Know how many rois have in the table == \\n \",\n",
    "                                dftable.year.value_counts())\n",
    "\n",
    "    dftableYY = dftable[dftable['year'] == yyear]\n",
    "    procFeatures_byYears.get_data(dftableYY, yyear)\n",
    "    lstKeysBa = [kk for kk in dictModelsS.keys()]\n",
    "    print(f\" basin activate {nbacia} and the other Regions {dftableYY.shape} processadas \\n ==> \", lstKeysBa)\n",
    "\n",
    "    if nbacia not in lstKeysBa:\n",
    "        procFeatures_byYears.processingMultiplesModels()\n",
    "        melhorModelo = procFeatures_byYears.betterPmtrosSet\n",
    "        dictModelsS[nbacia] = {\n",
    "            'better_pmtSet': melhorModelo,\n",
    "            'n_estimators': dictModelsS[str(melhorModelo)][0],\n",
    "            'learning_rate': dictModelsS[str(melhorModelo)][1]\n",
    "        }\n",
    "        # Convert and write JSON object to file\n",
    "        with open(pathModelJson, \"w\") as outfile:\n",
    "            os.system(f\"mkdir {pathModelJson}\") \n",
    "            # os.system(f'chmod 777 {pathModelJson}')\n",
    "            os.chmod(pathModelJson, 777)\n",
    "            # os.chmod(pathModelJson, 0o666)\n",
    "            json.dump(dictModelsS, outfile)\n",
    "\n",
    "    else:\n",
    "        df_bacia = dictModelsS[nbacia]\n",
    "        print(\"selecionou model feito \", df_bacia)\n",
    "        procFeatures_byYears.betterPmtrosSet = df_bacia['better_pmtSet']\n",
    "    # break\n",
    "    for nyear in lstYear:\n",
    "        dftableYY = dftable[dftable['year'] == nyear]\n",
    "        procFeatures_byYears.get_data(dftableYY, nyear)\n",
    "        procFeatures_byYears.get_better_featuresSet(fixarNFeat, numMin, nbacia, nyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UQpgeZEOHx0H",
   "metadata": {
    "id": "UQpgeZEOHx0H"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LRfSdjxyT9Pm",
   "metadata": {
    "id": "LRfSdjxyT9Pm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
