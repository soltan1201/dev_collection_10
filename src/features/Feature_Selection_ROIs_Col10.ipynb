{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41bd4272",
      "metadata": {
        "id": "41bd4272"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fvSYfDDhGr_G",
      "metadata": {
        "id": "fvSYfDDhGr_G"
      },
      "source": [
        "### imports packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bd0253",
      "metadata": {
        "id": "e1bd0253"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from numpy import set_printoptions\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from matplotlib.ticker import NullFormatter\n",
        "from sklearn import manifold, datasets\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d435efa",
      "metadata": {
        "id": "2d435efa"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SAMR3bGOjdCj",
      "metadata": {
        "id": "SAMR3bGOjdCj"
      },
      "source": [
        "### loading Data from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2FyL-Um8reAR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FyL-Um8reAR",
        "outputId": "503ba2ee-cfd0-4446-e307-6cde2de18932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uHoK4uvAjwHS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHoK4uvAjwHS",
        "outputId": "250437e1-6ffd-42b1-e4a8-0d2e3edda425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 >> /content/drive/MyDrive/ROIs_Joined_All/7615.csv\n",
            "#1 >> /content/drive/MyDrive/ROIs_Joined_All/7619.csv\n",
            "#2 >> /content/drive/MyDrive/ROIs_Joined_All/765.csv\n",
            "#3 >> /content/drive/MyDrive/ROIs_Joined_All/7712.csv\n",
            "#4 >> /content/drive/MyDrive/ROIs_Joined_All/773.csv\n",
            "#5 >> /content/drive/MyDrive/ROIs_Joined_All/7746.csv\n",
            "#6 >> /content/drive/MyDrive/ROIs_Joined_All/7438.csv\n",
            "#7 >> /content/drive/MyDrive/ROIs_Joined_All/752.csv\n",
            "#8 >> /content/drive/MyDrive/ROIs_Joined_All/7584.csv\n",
            "#9 >> /content/drive/MyDrive/ROIs_Joined_All/7591.csv\n",
            "#10 >> /content/drive/MyDrive/ROIs_Joined_All/761111.csv\n"
          ]
        }
      ],
      "source": [
        "path_base = '/content/drive/MyDrive/ROIs_Joined_All'\n",
        "pathFeaturesBase = '/content/drive/MyDrive/mapbiomas/featureSelection/feature_select_col10'\n",
        "lstfiles = glob.glob(path_base + '/*')\n",
        "lstpathfiles = []\n",
        "for cc, npath in enumerate(lstfiles):\n",
        "    if 'filtrada' not in npath:\n",
        "        print(f\"#{cc} >> {npath}\")\n",
        "        lstpathfiles.append(npath)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wyvBo8CuHAh8",
      "metadata": {
        "id": "wyvBo8CuHAh8"
      },
      "source": [
        "### Load Tables and paramenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v8Pq2ueXr4tz",
      "metadata": {
        "id": "v8Pq2ueXr4tz"
      },
      "outputs": [],
      "source": [
        "# read many files CSVs\n",
        "manyfile = False\n",
        "filterYear = True\n",
        "nyear = 2023\n",
        "# lstpathfiles = glob.glob(os.path.join(path_base, \"*.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diyoNFd9fiNP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diyoNFd9fiNP",
        "outputId": "21c7d78e-0a45-44ba-8565-71b021c053b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110\n"
          ]
        }
      ],
      "source": [
        "dftmp = pd.read_csv(lstpathfiles[4])\n",
        "dftmp = dftmp.drop(['GRID_ID','system:index','.geo'], axis=1)\n",
        "lstCol = list(dftmp.columns)\n",
        "print(len(lstCol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vs6UUNurhu7M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs6UUNurhu7M",
        "outputId": "0826fd5b-1976-4ef0-a3a2-f4da035eaa1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
            "\"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
            "\"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
            "\"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
            "\"bsi_median_2\",\"class\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
            "\"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
            "\"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
            "\"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
            "\"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
            "\"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
            "\"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
            "\"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
            "\"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
            "\"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
            "\"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
            "\"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
            "\"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
            "\"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
            "\"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
            "\"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
            "\"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
            "\"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\"year\",\n"
          ]
        }
      ],
      "source": [
        "text = ''\n",
        "for ii in range(0, len(lstCol)):\n",
        "    text = text + f'\"{lstCol[ii]}\",'\n",
        "    if ii % 5 == 0 and ii > 0:\n",
        "        print(text)\n",
        "        text = ''\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1HeNYU0t36Mi",
      "metadata": {
        "id": "1HeNYU0t36Mi"
      },
      "source": [
        "### Analises de Features Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eGLUTvyx1ooX",
      "metadata": {
        "id": "eGLUTvyx1ooX"
      },
      "outputs": [],
      "source": [
        "class processin_features_byYears(object):\n",
        "    columns_features = [\n",
        "        \"afvi_median\",\"afvi_median_dry\",\"afvi_median_wet\",\"avi_median\",\"avi_median_dry\",\"avi_median_wet\",\n",
        "        \"awei_median\",\"awei_median_dry\",\"awei_median_wet\",\"blue_median\",\"blue_median_dry\",\n",
        "        \"blue_median_wet\",\"blue_stdDev\",\"brba_median\",\"brba_median_dry\",\"brba_median_wet\",\n",
        "        \"brightness_median\",\"brightness_median_dry\",\"brightness_median_wet\",\"bsi_median\",\"bsi_median_1\",\n",
        "        \"bsi_median_2\",\"cvi_median\",\"cvi_median_dry\",\"cvi_median_wet\",\n",
        "        \"dswi5_median\",\"dswi5_median_dry\",\"dswi5_median_wet\",\"evi_median\",\"evi_median_dry\",\n",
        "        \"evi_median_wet\",\"gcvi_median\",\"gcvi_median_dry\",\"gcvi_median_wet\",\"gemi_median\",\n",
        "        \"gemi_median_dry\",\"gemi_median_wet\",\"gli_median\",\"gli_median_dry\",\"gli_median_wet\",\n",
        "        \"green_median\",\"green_median_dry\",\"green_median_wet\",\"green_stdDev\",\"gvmi_median\",\n",
        "        \"gvmi_median_dry\",\"gvmi_median_wet\",\"hillshade\",\"iia_median\",\"iia_median_dry\",\n",
        "        \"iia_median_wet\",\"lswi_median\",\"lswi_median_dry\",\"lswi_median_wet\",\"mbi_median\",\n",
        "        \"mbi_median_dry\",\"mbi_median_wet\",\"nddi_median\",\"nddi_median_dry\",\"nddi_median_wet\",\n",
        "        \"ndvi_median\",\"ndvi_median_dry\",\"ndvi_median_wet\",\"ndwi_median\",\"ndwi_median_dry\",\n",
        "        \"ndwi_median_wet\",\"nir_median\",\"nir_median_contrast\",\"nir_median_dry\",\"nir_median_dry_contrast\",\n",
        "        \"nir_median_wet\",\"nir_stdDev\",\"osavi_median\",\"osavi_median_dry\",\"osavi_median_wet\",\n",
        "        \"ratio_median\",\"ratio_median_dry\",\"ratio_median_wet\",\"red_median\",\"red_median_contrast\",\n",
        "        \"red_median_dry\",\"red_median_dry_contrast\",\"red_median_wet\",\"red_stdDev\",\"ri_median\",\n",
        "        \"ri_median_dry\",\"ri_median_wet\",\"rvi_median\",\"rvi_median_1\",\"rvi_median_wet\",\n",
        "        \"shape_median\",\"shape_median_dry\",\"shape_median_wet\",\"solpe\",\"swir1_median\",\n",
        "        \"swir1_median_dry\",\"swir1_median_wet\",\"swir1_stdDev\",\"swir2_median\",\"swir2_median_dry\",\n",
        "        \"swir2_median_wet\",\"swir2_stdDev\",\"ui_median\",\"ui_median_dry\",\"ui_median_wet\",\n",
        "        \"wetness_median\",\"wetness_median_dry\",\"wetness_median_wet\",\n",
        "    ]\n",
        "    classe = \"class\"\n",
        "\n",
        "    def __init__(self, Ns_estimators, learning_rates, path_features):\n",
        "        self.dfROIs = None\n",
        "        self.dfCC = None\n",
        "        self.yearAct = None\n",
        "        self.lstClass = None\n",
        "        self.lst_N_estimators = Ns_estimators\n",
        "        self.lst_learning_rate = learning_rates\n",
        "        self.path_features = path_features\n",
        "        self.betterPmtrosSet = 0\n",
        "        self.dictpmtGTB = {}\n",
        "        count = 0\n",
        "        for ne in self.lst_N_estimators:\n",
        "            for lr in self.lst_learning_rate:\n",
        "                self.dictpmtGTB[str(count)] = [ne, lr]\n",
        "                print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
        "                count += 1\n",
        "\n",
        "    def get_data(self, myDF, nYear):\n",
        "        self.dfROIs = myDF\n",
        "        self.yearAct = nYear\n",
        "        self.lstClass = self.dfROIs[self.classe].unique().tolist()\n",
        "        self.buildingPercentsofClass()\n",
        "\n",
        "    def get_class_withSmallsize(self, dFrames, lstSearch):\n",
        "        classeMin = []\n",
        "        for cclass in lstSearch:\n",
        "            nsize = dFrames[dFrames[self.classe] == cclass].shape[0]\n",
        "            print(f\" classe {cclass} == > size = {nsize}\")\n",
        "            if nsize < 4:\n",
        "                classeMin.append(cclass)\n",
        "        return classeMin\n",
        "\n",
        "    def split_dataFrame(self, dFrame):\n",
        "        # split data into inputs (X) and outputs (y)\n",
        "        dFrame4 = dFrame[dFrame[self.classe] == 4]\n",
        "        dFrameO = dFrame[dFrame[self.classe] != 4]\n",
        "        # lstClasses  = [kk for kk in self.lstClass if kk != 4]\n",
        "        addFeatext = False\n",
        "\n",
        "        maximoROIs = self.dfCC[self.dfCC['class'] != 4]['count'].max()\n",
        "        maximoROIs += 150\n",
        "        newlstDF = []\n",
        "        print(\"size dFrame4 \", dFrame4.shape, \" and the next class maximum is \", maximoROIs)\n",
        "        # sampled the N samples fro dataframe stratified\n",
        "        tmpDF = dFrame4.sample(n= int(maximoROIs), random_state= np.random.seed(int(maximoROIs/ 2)), replace= True)  #\n",
        "        concDF  = pd.concat([tmpDF, dFrameO], ignore_index=True) #\n",
        "        print(\"temos {} filas \".format(concDF.shape))\n",
        "        # concDF.head()\n",
        "        lstCCg1 = [3,4,15,18]\n",
        "        lstCCg2 = [12,21,22,33]\n",
        "        print(\" ====> analisando size of class smaller \")\n",
        "        lstclassMinM = self.get_class_withSmallsize(dFrame, lstCCg2)\n",
        "\n",
        "        if len(lstclassMinM)  > 0:\n",
        "            for ccm in lstclassMinM:\n",
        "                print(\" --- will be remove class ---\", ccm)\n",
        "                lstCCg2.remove(ccm)\n",
        "            addFeatext = True\n",
        "\n",
        "\n",
        "        dFrameg1 = concDF[concDF[self.classe].isin(lstCCg1)]\n",
        "        dFrameg2 = concDF[concDF[self.classe].isin(lstCCg2)]\n",
        "\n",
        "        # print(f\" adding {int(propCC * maximoROIs)} samples from class [{cclass}]\")\n",
        "        # X = dataFrame[self.columns_features[:]]\n",
        "        # y = dataFrame[self.classe]\n",
        "        X_traing1, X_testg1, y_traing1, y_testg1 = train_test_split(\n",
        "                            dFrameg1[self.columns_features[:]], dFrameg1[self.classe],\n",
        "                            train_size=0.05,\n",
        "                            random_state=1,\n",
        "                            shuffle=True,\n",
        "                            stratify = dFrameg1[self.classe]\n",
        "                        )\n",
        "        print(f\"colected Xtrain {X_traing1.shape[0]} | Xtest {X_testg1.shape[0]} | \" +\n",
        "                                f\"ytrain {y_traing1.shape[0]} | ytest {y_testg1.shape[0]}\")\n",
        "        X_traing2, X_testg2, y_traing2, y_testg2 = train_test_split(\n",
        "                            dFrameg2[self.columns_features[:]], dFrameg2[self.classe],\n",
        "                            train_size=0.9,\n",
        "                            random_state=1,\n",
        "                            shuffle=True,\n",
        "                            stratify = dFrameg2[self.classe]\n",
        "                        )\n",
        "        print(f\"colected Xtrain {X_traing2.shape[0]} | Xtest {X_testg2.shape[0]} | \" +\n",
        "                                f\"ytrain {y_traing2.shape[0]} | ytest {y_testg2.shape[0]}\")\n",
        "\n",
        "\n",
        "        self.X_train = pd.concat([X_traing1, X_traing2], ignore_index=True)\n",
        "        self.X_test = pd.concat([X_testg1, X_testg2], ignore_index=True)\n",
        "        self.y_train = pd.concat([y_traing1, y_traing2], ignore_index=True)\n",
        "        self.y_test = pd.concat([y_testg1, y_testg2], ignore_index=True)\n",
        "\n",
        "        print(f\" ==== know we have {self.X_train.shape} to train ==== \")\n",
        "        print(self.y_train.value_counts(normalize= True), self.y_train.value_counts())\n",
        "\n",
        "\n",
        "    def buildingPercentsofClass(self):\n",
        "        self.dfCC = self.dfROIs['class'].value_counts()\n",
        "        self.dfCC = self.dfCC.reset_index()\n",
        "        # get total number of classes\n",
        "        total = np.sum(self.dfCC['count'].tolist())\n",
        "        print(f\"total = {total}\")\n",
        "        self.dfCC['percent'] = round((self.dfCC['count'] * 100)/ total, 2)\n",
        "        self.dfCC['Years'] = np.ones(self.dfCC.shape[0]).astype(int) * self.yearAct\n",
        "        # print(dfCC)\n",
        "        print(f\" == the CLASS of rois distribuided in {self.yearAct} are == \\n \", self.dfCC)\n",
        "\n",
        "    def processingMultiplesModels(self):\n",
        "        # split the dataframe in stratify samples by class and balance class 4\n",
        "        self.split_dataFrame(self.dfROIs)\n",
        "        maximAcc = 0.0\n",
        "        # get the models to evaluate\n",
        "        models = self.get_models()\n",
        "        # evaluate the models and store results\n",
        "        results, names = list(), list()\n",
        "        start = time.time()\n",
        "        count = 1\n",
        "        for name, model in models.items():\n",
        "            print(f\"#{count}/{len(models.items())} processing model {name}\")\n",
        "            scores = self.evaluate_model(model, self.X_train[self.columns_features[:]], self.y_train)\n",
        "            results.append(scores)\n",
        "            names.append(name)\n",
        "            print('  >%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
        "            if maximAcc < np.mean(scores):\n",
        "                self.betterPmtrosSet = count - 1\n",
        "                maximAcc = np.mean(scores)\n",
        "            count += 1\n",
        "        # plot model performance for comparison\n",
        "        # plt.boxplot(results, labels=names, showmeans=True)\n",
        "        # plt.show()\n",
        "        end = time.time()\n",
        "        tiempo = end - start\n",
        "        if tiempo < 60:\n",
        "            print(f\"model trained in {tiempo} seconds\")\n",
        "        else:\n",
        "            print(f\"model trained in {tiempo/60} minutos\")\n",
        "\n",
        "\n",
        "    # get a list of models to evaluate\n",
        "    def get_models(self):\n",
        "        min_features_to_select = 7\n",
        "        models = dict()\n",
        "\n",
        "        # criando pipeline do modelos gradiente Boosting com varios paramentros\n",
        "        cv = StratifiedKFold(3)\n",
        "        for cc in range(len(self.dictpmtGTB.keys())):\n",
        "            GTBmodel = GradientBoostingClassifier(\n",
        "                            n_estimators= self.dictpmtGTB[str(cc)][0],\n",
        "                            learning_rate= self.dictpmtGTB[str(cc)][1],\n",
        "                            max_features= 7\n",
        "                        )\n",
        "            rfe = RFECV(\n",
        "                    estimator=GTBmodel,\n",
        "                    step=1,\n",
        "                    cv=cv,\n",
        "                    scoring=\"accuracy\",\n",
        "                    min_features_to_select=min_features_to_select,\n",
        "                    n_jobs= -1,\n",
        "                )\n",
        "\n",
        "            models[str(cc)] = Pipeline(steps=[('s', rfe),('m', GTBmodel)])\n",
        "        return models\n",
        "\n",
        "    # evaluate a give model using cross-validation\n",
        "    def evaluate_model(self, model, X, y):\n",
        "        cv = StratifiedKFold(3)\n",
        "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv,  error_score='raise') # n_jobs=-1,\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def get_better_featuresSet(self, fixarNumbFeat, numbMin, regg, yyear):\n",
        "\n",
        "        # Create a Path object\n",
        "        path_name_file = self.path_features + f'/featuresSelectS2_{regg}_{yyear}.csv'\n",
        "        file_path = Path(path_name_file)\n",
        "\n",
        "        # Check if the file exists\n",
        "        if file_path.exists():\n",
        "            print(\" ******* list of features selected was saved ********\")\n",
        "\n",
        "        else:\n",
        "            self.split_dataFrame(self.dfROIs)\n",
        "\n",
        "            GTBmodel = GradientBoostingClassifier(\n",
        "                            n_estimators= self.dictpmtGTB[str(self.betterPmtrosSet)][0],\n",
        "                            learning_rate= self.dictpmtGTB[str(self.betterPmtrosSet)][1],\n",
        "                            max_features= 7,\n",
        "                            random_state=42\n",
        "                        )\n",
        "\n",
        "            start = time.time()\n",
        "            # Minimum number of features to consider\n",
        "            min_features_to_select =  7\n",
        "            cv = StratifiedKFold(3)\n",
        "            rfecv = RFECV(\n",
        "                estimator=GTBmodel,\n",
        "                step=1,\n",
        "                cv=cv,\n",
        "                scoring=\"accuracy\",\n",
        "                min_features_to_select=min_features_to_select,\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "            rfecv.fit(self.X_train[self.columns_features], self.y_train)\n",
        "            print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
        "\n",
        "            end = time.time()\n",
        "            tiempo = end - start\n",
        "            if tiempo < 60:\n",
        "                print(f\"model trained in {tiempo} seconds\")\n",
        "            else:\n",
        "                print(f\"model trained in {tiempo/60} minutos\")\n",
        "\n",
        "            ## valores ótimos aparecem com valor 1 no ranking\n",
        "            lst_ranking = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < 2]\n",
        "            print(\"quantos features otimos \", len(lst_ranking))\n",
        "            numbNotOti = numbMin\n",
        "            if fixarNumbFeat and numbMin > len(lst_ranking):\n",
        "                numbNotOti = numbMin - len(lst_ranking)\n",
        "                print(f\"Addicionando << {numbNotOti} >> features a mais não ótimas \")\n",
        "                lst_ranking = [(kk, cc) for cc, kk in enumerate(rfecv.ranking_) if kk < numbNotOti]\n",
        "            print(\"quantos features otimos \", len(lst_ranking))\n",
        "\n",
        "            lstFeatSelect = []\n",
        "            ccount = 1\n",
        "            for kk, cc in lst_ranking:\n",
        "                print(f\"# {ccount} ranking {kk} | pos {cc} >> feature >> {self.columns_features[cc]}\")\n",
        "                lstFeatSelect.append(self.columns_features[cc])\n",
        "                ccount += 1\n",
        "\n",
        "            dict_result= {\n",
        "                'ranking': lst_ranking,\n",
        "                'features': lstFeatSelect\n",
        "            }\n",
        "            dfresult = pd.DataFrame.from_dict(dict_result)\n",
        "            dfresult.to_csv(path_name_file, index= False)\n",
        "\n",
        "            print(\"tabela salva em pasta do drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yW0vlol_H1VQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW0vlol_H1VQ",
        "outputId": "890db644-5879-41a2-d0cf-7b843f85ffc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " year  [2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998, 1997, 1996, 1995, 1994, 1993, 1992, 1991, 1990, 1989, 1988, 1987, 1986]\n"
          ]
        }
      ],
      "source": [
        "lstYear = list(range(2024, 1985, -1))\n",
        "print(\" year \", lstYear);\n",
        "# sys.exit()\n",
        "# fixar o número de variaveis\n",
        "fixarNFeat = True\n",
        "# número máximo de variaveis para o modelo das 144\n",
        "numMin = 70\n",
        "yyear = 2023\n",
        "lstEstimadors = [15, 20, 30, 40, 50, 60]\n",
        "lstLearnRate = [0.001, 0.005, 0.01, 0.1]\n",
        "melhorModelo = 0\n",
        "dictModel = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0AXgPLMGfarP",
      "metadata": {
        "id": "0AXgPLMGfarP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0UGNM1nFhwuT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UGNM1nFhwuT",
        "outputId": "b522391d-fea7-4a16-a161-eb4913da41ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
            "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
            "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
            "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
            "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
            "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
            "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
            "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
            "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
            "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
            "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
            "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
            "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
            "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
            "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
            "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
            "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
            "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
            "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
            "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
            "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
            "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
            "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
            "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n"
          ]
        }
      ],
      "source": [
        "# instanciar classe de processamento dos Features\n",
        "procFeatures_byYears = processin_features_byYears(lstEstimadors, lstLearnRate, pathFeaturesBase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i5tqkc1mh8mn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5tqkc1mh8mn",
        "outputId": "6edca6a6-d398-4d1b-87e0-2e7dd4a572b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generate dictModel => {}\n",
            "# 1 mudando n_estimators= 15 & learning_rate= 0.001\n",
            "# 2 mudando n_estimators= 15 & learning_rate= 0.005\n",
            "# 3 mudando n_estimators= 15 & learning_rate= 0.01\n",
            "# 4 mudando n_estimators= 15 & learning_rate= 0.1\n",
            "# 5 mudando n_estimators= 20 & learning_rate= 0.001\n",
            "# 6 mudando n_estimators= 20 & learning_rate= 0.005\n",
            "# 7 mudando n_estimators= 20 & learning_rate= 0.01\n",
            "# 8 mudando n_estimators= 20 & learning_rate= 0.1\n",
            "# 9 mudando n_estimators= 30 & learning_rate= 0.001\n",
            "# 10 mudando n_estimators= 30 & learning_rate= 0.005\n",
            "# 11 mudando n_estimators= 30 & learning_rate= 0.01\n",
            "# 12 mudando n_estimators= 30 & learning_rate= 0.1\n",
            "# 13 mudando n_estimators= 40 & learning_rate= 0.001\n",
            "# 14 mudando n_estimators= 40 & learning_rate= 0.005\n",
            "# 15 mudando n_estimators= 40 & learning_rate= 0.01\n",
            "# 16 mudando n_estimators= 40 & learning_rate= 0.1\n",
            "# 17 mudando n_estimators= 50 & learning_rate= 0.001\n",
            "# 18 mudando n_estimators= 50 & learning_rate= 0.005\n",
            "# 19 mudando n_estimators= 50 & learning_rate= 0.01\n",
            "# 20 mudando n_estimators= 50 & learning_rate= 0.1\n",
            "# 21 mudando n_estimators= 60 & learning_rate= 0.001\n",
            "# 22 mudando n_estimators= 60 & learning_rate= 0.005\n",
            "# 23 mudando n_estimators= 60 & learning_rate= 0.01\n",
            "# 24 mudando n_estimators= 60 & learning_rate= 0.1\n"
          ]
        }
      ],
      "source": [
        "dictModelsS = {}\n",
        "pathModelJson = '/content/drive/MyDrive/mapbiomas/featureSelection/dictBetterModelpmtCol10v1.json'\n",
        "try:\n",
        "    with open(pathModelJson, 'r') as fh:\n",
        "        dictModelsS = json.load(fh)\n",
        "    print(\"loaded dictModel => \", dictModelsS)\n",
        "    print(f\"with {len(dictModelsS.keys())} register\")\n",
        "except:\n",
        "    print(\"generate dictModel => {}\")\n",
        "\n",
        "count = 0\n",
        "for ne in lstEstimadors:\n",
        "    for lr in lstLearnRate:\n",
        "        dictModelsS[str(count)] = [ne, lr]\n",
        "        print(f\"# {count + 1} mudando n_estimators= {ne} & learning_rate= {lr}\")\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjCdWlyRoP3K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjCdWlyRoP3K",
        "outputId": "98590d54-7809-4df0-a8ac-ef8812e2a39f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': [15, 0.001],\n",
              " '1': [15, 0.005],\n",
              " '2': [15, 0.01],\n",
              " '3': [15, 0.1],\n",
              " '4': [20, 0.001],\n",
              " '5': [20, 0.005],\n",
              " '6': [20, 0.01],\n",
              " '7': [20, 0.1],\n",
              " '8': [30, 0.001],\n",
              " '9': [30, 0.005],\n",
              " '10': [30, 0.01],\n",
              " '11': [30, 0.1],\n",
              " '12': [40, 0.001],\n",
              " '13': [40, 0.005],\n",
              " '14': [40, 0.01],\n",
              " '15': [40, 0.1],\n",
              " '16': [50, 0.001],\n",
              " '17': [50, 0.005],\n",
              " '18': [50, 0.01],\n",
              " '19': [50, 0.1],\n",
              " '20': [60, 0.001],\n",
              " '21': [60, 0.005],\n",
              " '22': [60, 0.01],\n",
              " '23': [60, 0.1]}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dictModelsS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KwKMr4zFH0dM",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwKMr4zFH0dM",
        "outputId": "3837b2e1-32af-4292-eac6-af3d394b2352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ==> /content/drive/MyDrive/ROIs_Joined_All/7615.csv\n"
          ]
        }
      ],
      "source": [
        "for cc, npath in enumerate(lstpathfiles[:]):\n",
        "    df_bacia = {}\n",
        "    print(\"Loading ==> \" + npath)\n",
        "    nbacia = npath.split(\"/\")[-1].replace('.csv', '')\n",
        "    dftable = pd.read_csv(npath)\n",
        "    dftable = dftable.drop(['system:index','.geo'], axis=1)\n",
        "    print(\"  columns = \", dftable.columns)\n",
        "    print(f\" == Know how many rois have in the table == \\n \",\n",
        "                                dftable.year.value_counts())\n",
        "\n",
        "    dftableYY = dftable[dftable['year'] == yyear]\n",
        "    procFeatures_byYears.get_data(dftableYY, yyear)\n",
        "    lstKeysBa = [kk for kk in dictModelsS.keys()]\n",
        "    print(f\" basin activate {nbacia} and the other Regions {dftableYY.shape} processadas \\n ==> \", lstKeysBa)\n",
        "\n",
        "    if nbacia not in lstKeysBa:\n",
        "        procFeatures_byYears.processingMultiplesModels()\n",
        "        melhorModelo = procFeatures_byYears.betterPmtrosSet\n",
        "        dictModelsS[nbacia] = {\n",
        "            'better_pmtSet': melhorModelo,\n",
        "            'n_estimators': dictModel[str(melhorModelo)][0],\n",
        "            'learning_rate': dictModel[str(melhorModelo)][1]\n",
        "        }\n",
        "        # Convert and write JSON object to file\n",
        "        with open(pathModelJson, \"w\") as outfile:\n",
        "            json.dump(dictModelsS, outfile)\n",
        "\n",
        "    else:\n",
        "        df_bacia = dictModelsS[nbacia]\n",
        "        print(\"selecionou model feito \", df_bacia)\n",
        "        procFeatures_byYears.betterPmtrosSet = df_bacia['better_pmtSet']\n",
        "    # break\n",
        "    for nyear in lstYear:\n",
        "        dftableYY = dftable[dftable['year'] == nyear]\n",
        "        procFeatures_byYears.get_data(dftableYY, nyear)\n",
        "        procFeatures_byYears.get_better_featuresSet(fixarNFeat, numMin, nbacia, nyear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UQpgeZEOHx0H",
      "metadata": {
        "id": "UQpgeZEOHx0H"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LRfSdjxyT9Pm",
      "metadata": {
        "id": "LRfSdjxyT9Pm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}